# ============================================
# Algorithms From Scratch â€“ Theory + Example
# Author: Prajwal Mavkar
# ============================================

import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

# ============================================
# 1. LINEAR REGRESSION (FROM SCRATCH)
# ============================================

class LinearRegressionScratch:
    def __init__(self, lr=0.01, epochs=1000):
        self.lr = lr
        self.epochs = epochs

    def fit(self, X, y):
        self.m, self.n = X.shape
        self.W = np.zeros(self.n)
        self.b = 0

        for _ in range(self.epochs):
            y_pred = np.dot(X, self.W) + self.b
            dw = (1/self.m) * np.dot(X.T, (y_pred - y))
            db = (1/self.m) * np.sum(y_pred - y)

            self.W -= self.lr * dw
            self.b -= self.lr * db

    def predict(self, X):
        return np.dot(X, self.W) + self.b


# ============================================
# 2. LOGISTIC REGRESSION (FROM SCRATCH)
# ============================================

class LogisticRegressionScratch:
    def __init__(self, lr=0.01, epochs=1000):
        self.lr = lr
        self.epochs = epochs

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        self.m, self.n = X.shape
        self.W = np.zeros(self.n)
        self.b = 0

        for _ in range(self.epochs):
            z = np.dot(X, self.W) + self.b
            y_pred = self.sigmoid(z)

            dw = (1/self.m) * np.dot(X.T, (y_pred - y))
            db = (1/self.m) * np.sum(y_pred - y)

            self.W -= self.lr * dw
            self.b -= self.lr * db

    def predict(self, X):
        z = np.dot(X, self.W) + self.b
        return (self.sigmoid(z) >= 0.5).astype(int)


# ============================================
# 3. K-MEANS CLUSTERING (FROM SCRATCH)
# ============================================

class KMeansScratch:
    def __init__(self, k=3, epochs=100):
        self.k = k
        self.epochs = epochs

    def fit(self, X):
        self.centroids = X[np.random.choice(len(X), self.k, replace=False)]

        for _ in range(self.epochs):
            clusters = [[] for _ in range(self.k)]

            for point in X:
                distances = [np.linalg.norm(point - c) for c in self.centroids]
                cluster_idx = np.argmin(distances)
                clusters[cluster_idx].append(point)

            self.centroids = [np.mean(cluster, axis=0) for cluster in clusters]

        return self.centroids


# ============================================
# 4. PRINCIPAL COMPONENT ANALYSIS (PCA)
# ============================================

class PCAScratch:
    def __init__(self, n_components):
        self.n_components = n_components

    def fit_transform(self, X):
        X_meaned = X - np.mean(X, axis=0)
        cov_matrix = np.cov(X_meaned, rowvar=False)

        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        sorted_idx = np.argsort(eigenvalues)[::-1]
        eigenvectors = eigenvectors[:, sorted_idx]

        components = eigenvectors[:, :self.n_components]
        return np.dot(X_meaned, components)


# ============================================
# 5. DECISION TREE (BASIC VERSION)
# ============================================

class DecisionTreeScratch:
    def __init__(self, depth=3):
        self.depth = depth

    def entropy(self, y):
        counts = Counter(y)
        entropy = 0
        for count in counts.values():
            p = count / len(y)
            entropy -= p * np.log2(p)
        return entropy

    def fit(self, X, y):
        self.feature = np.argmax([np.var(X[:, i]) for i in range(X.shape[1])])
        self.threshold = np.mean(X[:, self.feature])

        self.left_class = Counter(y[X[:, self.feature] <= self.threshold]).most_common(1)[0][0]
        self.right_class = Counter(y[X[:, self.feature] > self.threshold]).most_common(1)[0][0]

    def predict(self, X):
        return np.array([
            self.left_class if x[self.feature] <= self.threshold else self.right_class
            for x in X
        ])


# ============================================
# 6. EVALUATION METRICS
# ============================================

def accuracy(y_true, y_pred):
    return np.sum(y_true == y_pred) / len(y_true)

def precision(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    return tp / (tp + fp + 1e-9)

def recall(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    return tp / (tp + fn + 1e-9)


# ============================================
# 7. EXAMPLE RUN
# ============================================

if __name__ == "__main__":
    # Sample data
    X = np.array([[1], [2], [3], [4], [5]])
    y = np.array([1, 3, 5, 7, 9])

    # Linear Regression
    lr = LinearRegressionScratch()
    lr.fit(X, y)
    print("Linear Regression Prediction:", lr.predict(np.array([[6]])))

    # Logistic Regression
    X_log = np.array([[1], [2], [3], [4]])
    y_log = np.array([0, 0, 1, 1])
    log_reg = LogisticRegressionScratch()
    log_reg.fit(X_log, y_log)
    print("Logistic Regression Prediction:", log_reg.predict(np.array([[3]])))

    # K-Means
    X_cluster = np.random.rand(100, 2)
    kmeans = KMeansScratch(k=3)
    centroids = kmeans.fit(X_cluster)
    print("K-Means Centroids:", centroids)

    # PCA
    X_pca = np.random.rand(50, 5)
    pca = PCAScratch(n_components=2)
    reduced = pca.fit_transform(X_pca)
    print("PCA Output Shape:", reduced.shape)

    # Decision Tree
    X_tree = np.array([[2], [4], [6], [8]])
    y_tree = np.array([0, 0, 1, 1])
    tree = DecisionTreeScratch()
    tree.fit(X_tree, y_tree)
    print("Decision Tree Prediction:", tree.predict(np.array([[5]])))
